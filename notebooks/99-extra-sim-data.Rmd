---
author: "Maciej Beręsewicz"
title: "Symulacje danych z rozkładu dwumianoweg"
output: html_notebook
---

# Pakiety

```{r}
library(car)
library(mlogit)
library(tidyr)
library(maxLik)
library(optimParallel)
library(nloptr)
```

# Symulacja danych

## Symulujemy z rozkładu dwumianowego

Generujemy zmienną Y

$$
Pr(Y == 1) = \frac{\exp(\beta_0 + \beta_1x_1 + \beta_2x_2)}{1 + \exp(\beta_0 + \beta_1x_1 + \beta_2x_2)} = \frac{\exp(\eta)}{1+\exp(\eta)}
$$

Generujemy indykator przynależności do próby (response indicator)

$$
Pr(R == 1) =  \frac{\exp(\gamma_0 + \gamma_1Y)}{1 + \exp(\gamma_0 + \gamma_1Y)} = \frac{\exp(\zeta)}{1 + \exp(\zeta)}
$$
```{r}
set.seed(123)

## target variable
n <- 10000
x1 <- rnorm(n, mean = 2, sd = 0.5)
x2 <- rchisq(n = n, df = 3)
beta0 <- 1.5
beta1 <- -0.5
beta2 <- -0.5
eta <- beta0 + beta1*x1 + beta2*x2
y <- rbinom(n, size = 1, prob = exp(eta) / (1 + exp(eta)))
table(y)

## response indicator
gamma0 <- -1.5
gamma1 <- 1.5
zeta <- gamma0 + gamma1*y
resp <- rbinom(n, size = 1, prob = exp(zeta)/(1 + exp(zeta)))
table(resp)

## crosstab
xtabs(~ resp + y)
```

sample data 

```{r}
population <- data.frame(y = y, x1, x2, resp)
sample <- subset(population, resp == 1)
#sample size 
nrow(sample) / n
## naive estimate
mean(sample$y)
## true value
mean(population$y)
```

Sprawdzmy czy nasze dane populacji odtwarzają parametry

```{r}
model_pop <- glm(y ~ x1 + x2, data = population, family = binomial())
model_sam <- glm(y ~ x1 + x2, data = sample, family = binomial())
```

```{r}
compareCoefs(model_pop, model_sam, se = F)
```

Jeżeli nie uwzględnimy sample selection to otrzymujemy obciążone oszacowania

## Rozkład wielomianowy 

$$
Pr(Y_k == 1) = \frac{\exp(\beta_{k0} + \beta_{k1}x_1 + \beta_{k2}x_2)}{1 + \sum_{K}^{K-1}\exp(\beta_{k0} + \beta_{k1}x_1 + \beta_{k2}x_2)} = \frac{\exp(\eta_k)}{1+\sum_{k}^{K-1}\exp(\eta_k)}
$$


Tworzymy teraz response indicator, który bedzie ujemnie związany z brakiem social media

$$
Pr(R == 1) =  \frac{\exp(\gamma_0 + \gamma_1Y_2 + \gamma_2Y_1)}{1 + \exp(\gamma_0 + \gamma_1Y_2 + \gamma_2Y_1)} = \frac{\exp(\zeta)}{1 + \exp(\zeta)}
$$



Załóżmy, że mamy trzy kategorie (i.e Twitter, no social media, Facebook)

```{r}
set.seed(666)

n <- 10000
x1 <- rbinom(n = n, size = 1, prob = 0.7)
x2 <- rbinom(n = n, size = 1, prob = 0.2)

beta_02 <- -1
beta_03 <- -1

beta_12 <- 2
beta_13 <- 3

beta_22 <- 2
beta_23 <- 1

eta_1 <- 1
eta_2 <- beta_02 + beta_12*x1  + beta_22*x2
eta_3 <- beta_03 + beta_13*x1  + beta_23*x2


p2 <- exp(eta_2) / (1 + exp(eta_2) + exp(eta_3))
p3 <- exp(eta_3) / (1 + exp(eta_2) + exp(eta_3))
p1 <- 1-(p2+p3)

probs <- cbind(p1,p2,p3)

d <- apply(probs, MARGIN = 1, FUN = function(x) rmultinom(n = 1, size = 1, prob = x))
d <- t(d)
df <- cbind(d, x1, x2)
df <- as.data.frame(df)
colnames(df) <- c("y1","y2","y3", "x1", "x2")
df$id <- 1:nrow(df)
df$y <- ifelse(df$y1 == 1, 1, ifelse(df$y2 == 1, 2, 3))
table(df$y)

gamma0 <- 0.5
gamma1 <- -1
gamma2 <- 2
zeta <- gamma0 + gamma1*(df$y2) #+ gamma2*df$y2
resp <- rbinom(n, size = 1, prob = exp(zeta)/(1 + exp(zeta)))
table(resp)
df$zeta <- zeta
df$resp <- resp

xtabs(~resp+y,df)


```




## Estymacja modelu wielomianowego 

Sprawdzmy czy poprawnie odtworzymy parametry -- z wykorzystaniem pakietu NNET

```{r}
coef(summary(nnet::multinom(y ~ x1 + x2, data = df, trace = F)))

coef(summary(nnet::multinom(y ~ x1 + x2, data = subset(df, resp == 1), trace = F)))

```

To samo ale z wykorzystaniem pakietu mlogit

```{r} 
df_mlogit <- gather(df, alt, value, y1:y3) %>% dplyr::arrange(id)
head(df_mlogit)

mlogit_model1 <- mlogit(value ~ 0 | x1 + x2, data = subset(df_mlogit, resp == 1), 
                        chid.var = "id",  alt.var="alt", choice = "value",  shape = "long")

summary(mlogit_model1)
```

## Model 1 -- zwykła regresja wielomianowa
 
$$
\log L_1(\beta) = \sum_{t=1}^T \sum_{j=1}^{K} y_{tj} \log 
\left( \frac{\exp(x_{jt}'\beta)}{\sum_{j=1}^5 \exp(x_{jt}'\beta)} \right) 
$$

Gradient

$$
\frac{\partial \log L_1(\beta)}{\partial \beta_j} = \sum_{t=1}^T 
\left(y_{tj}-\frac{\exp(x_{tj}'\beta_j)}{\sum_{k=1}^K \exp(x_{tk}'\beta_k)}
\right)x_{tj}
$$


```{r}
## likelihood function
model1_ll <- function(par, df) {
  
  X <- cbind(1, df$x1, df$x2)
  eta_1 <- 1
  eta_2 <- X %*% par[1:3]
  eta_3 <- X %*% par[4:6]
  etas <- cbind(eta_1, eta_2, eta_3)
  #eta_den <- (1 + exp(eta_2) + exp(eta_3))
  
  p2 <- exp(eta_2) / (1 + exp(eta_2) + exp(eta_3))
  p3 <- exp(eta_3) / (1 + exp(eta_2) + exp(eta_3))
  p1 <- 1 - (p2 + p3)
  
  probs <- cbind(p1, p2, p3)
  ll <- sum(df[,c("y1", "y2","y3")]*log(probs))
  return(ll)
}

model1_ll_grad <-  function(par, df) {
  
  X <- cbind(1, df$x1, df$x2)
  eta_2 <- X %*% par[1:3]
  eta_3 <- X %*% par[4:6]
  
  p2 <- exp(eta_2) / (1 + exp(eta_2) + exp(eta_3))
  p3 <- exp(eta_3) / (1 + exp(eta_2) + exp(eta_3))
  #p1 <- 1 - (p2 + p3)
  
  g1 <- colSums(as.vector(df$y2 - p2) * X)
  g2 <- colSums(as.vector(df$y3 - p3) * X)
  
  return(c(g1,g2))
}

model1_result <- maxLik(logLik = model1_ll, 
                        grad = model1_ll_grad,
                        start = rep(1,6), 
                        df = df, 
                        control=list(printLevel=1),
                        method = "NR")

summary(model1_result)

```


Model 2

$$
\log L_2(\beta,\alpha) = \sum_{t=1}^T\left\{
\sum_{j=1}^5y_{tj}\left(
x_{tj}'\beta_j - \log(1+\alpha) - \log\left(\sum_{i=1}^5\exp(x_{tj}'\beta)\right)
\right) +
y_{tm}(\log(\alpha) - \log(1+\alpha))
\right\}
$$


```{r}
model2_ll <- function(par, df) {
  
  ## flags
  y_obs <- df$resp==1
  y_miss <- df$resp==0
  
  X <- cbind(1, df$x1, df$x2)
  eta_1 <- 1
  eta_2 <- X %*% par[1:3]
  eta_3 <- X %*% par[4:6]
  etas <- cbind(eta_1, eta_2, eta_3)
  alpha <- par[7]^2
  eta_denom <- as.numeric(1 + exp(eta_2) + exp(eta_3))
  
  ll_obs <- rowSums(df[,c("y1", "y2","y3")]*(etas - log(1+alpha) - log(eta_denom)))
  ll_miss <- log(alpha)-log(1+alpha)
  ll <- sum(ll_obs*y_obs + ll_miss*y_miss)
  
  return(ll)

}


# model2_ll_grad <-  function(par, df) {
#   
#   y_obs <- df$sel == 1
#   y_miss <- df$sel == 0
#   
#   X <- cbind(1, df$x1, df$x2)
#   eta_2 <- X %*% par[1:3]
#   eta_3 <- X %*% par[4:6]
#   alpha <- par[7]^2
#   eta_denom <- as.numeric(1 + exp(eta_2) + exp(eta_3))
#   
#   p2 <- exp(eta_2) / eta_denom
#   p3 <- exp(eta_3) / eta_denom
#   #p1 <- 1 - (p2 + p3)
#   
#   ## ll observed
#   g1 <- colSums(as.vector(df$y2*(1- p2))*X*y_obs)
#   g2 <- colSums(as.vector(df$y2*(1- p3))*X*y_obs)
#   
#   ## alpha
#   ## ta w
#   g3 <- sum(rowSums(df[,c("y1","y2","y3")]*(-1)/(1+alpha))*y_obs + y_miss*(1/alpha-1/(1+alpha)))
#   
#   return(c(g1,g2,g3))
# }


model2_result <- maxLik(logLik = model2_ll,
                        #grad =  model2_ll_grad,
                        start = c(rep(0,6), 0.5),
                        df = df,
                        control = list(printLevel = 2),
                        method = "BFGS")

summary(model2_result)

```

## Model 3


```{r}
par <- c(rep(1,6),rep(1,3))

model3_ll <- function(par, df) {
  
  ## flags
  y_obs <- df$resp==1
  y_miss <- df$resp==0
  
  X <- cbind(1, df$x1, df$x2)
  Y <- df[,c("y1","y2","y3")]*y_obs
  
  eta_1 <- 1
  eta_2 <- X %*% par[1:3]
  eta_3 <- X %*% par[4:6]
  etas <- cbind(eta_1, eta_2, eta_3)
  sumex <- as.numeric(eta_1 + exp(eta_2) + exp(eta_3))
  
  d1 <- par[7]*par[7]
  d2 <- par[8]*par[8]
  d3 <- par[9]*par[9]
  
  sumcexc <- (d1 * eta_1 / (1 +d1)) + (d2 * exp(eta_2) / (1 + d2)) + (d3 *exp(eta_3) / (1 + d3))
  
  loglike <- y_miss*log(sumcexc)  + 
    Y[,1]*(eta_1 - log(1+d1)) +  Y[,2]*(eta_2 - log(1+d2)) + Y[,3]*(eta_3 - log(1+d3)) - 
    log(sumex)
  
  ll <- sum(loglike)
  return(ll)

}

```

```{r}
system.time(
  model3_result <- maxLik(logLik = model3_ll,
                        start = c(rep(0,6), rep(1,3)),
                        df = df,
                        #control = list(printLevel = 2),
                        method = "BFGS")
)

summary(model3_result)

## BFGS: -1.0333200  2.0807783  2.1944498 -0.9184809  3.0582155  1.0808541  0.7354402  1.2310429  0.8057377
## NR:   -0.8605197  2.0767503  2.1230154 -0.9431037  3.0989761  1.0979990  0.7211160  1.3355608 -0.7657679
```


With optimParallel


Model 3

```{r}
model3_negll <- function(par, df) {
  
  ## flags
  y_obs <- df$resp==1
  y_miss <- df$resp==0
  
  X <- cbind(1, df$x1, df$x2)
  Y <- df[,c("y1","y2","y3")]*y_obs
  
  eta_1 <- 1
  eta_2 <- X %*% par[1:3]
  eta_3 <- X %*% par[4:6]
  etas <- cbind(eta_1, eta_2, eta_3)
  sumex <- as.numeric(eta_1 + exp(eta_2) + exp(eta_3))
  
  d1 <- par[7]*par[7]
  d2 <- par[8]*par[8]
  d3 <- par[9]*par[9]
  
  sumcexc <- (d1 * eta_1 / (1 +d1)) + (d2 * exp(eta_2) / (1 + d2)) + (d3 *exp(eta_3) / (1 + d3))
  
  loglike <- y_miss*log(sumcexc)  + 
    Y[,1]*(eta_1 - log(1+d1)) +  Y[,2]*(eta_2 - log(1+d2)) + Y[,3]*(eta_3 - log(1+d3)) - 
    log(sumex)
  
  ll <- sum(loglike)
  return(-ll)

}

model1_negll <- function(par, df) {
  
  X <- cbind(1, df$x1, df$x2)
  eta_1 <- 1
  eta_2 <- X %*% par[1:3]
  eta_3 <- X %*% par[4:6]
  etas <- cbind(eta_1, eta_2, eta_3)
  #eta_den <- (1 + exp(eta_2) + exp(eta_3))
  
  p2 <- exp(eta_2) / (1 + exp(eta_2) + exp(eta_3))
  p3 <- exp(eta_3) / (1 + exp(eta_2) + exp(eta_3))
  p1 <- 1 - (p2 + p3)
  
  probs <- cbind(p1, p2, p3)
  ll <- sum(df[,c("y1", "y2","y3")]*log(probs))
  return(-ll)
}

model1_negll_grad <-  function(par, df) {
  
  X <- cbind(1, df$x1, df$x2)
  eta_2 <- X %*% par[1:3]
  eta_3 <- X %*% par[4:6]
  
  p2 <- exp(eta_2) / (1 + exp(eta_2) + exp(eta_3))
  p3 <- exp(eta_3) / (1 + exp(eta_2) + exp(eta_3))
  #p1 <- 1 - (p2 + p3)
  
  g1 <- colSums(as.vector(df$y2 - p2) * X)
  g2 <- colSums(as.vector(df$y3 - p3) * X)
  
  return(-c(g1,g2))
}



```



```{r}
cl <- makeCluster(8)     # set the number of processor cores
setDefaultCluster(cl=cl) 
res <- optimParallel(fn = model3_negll, 
                     #gr = model1_ll_grad,
                     par =  c(rep(0,6),rep(1,3)),
                     df = df, 
                     hessian = TRUE)

# pars -0.9623010  1.9616269  2.1609346 -0.9690558  2.9752662  1.1278755
## 0.04812783 0.06777400 0.09902509 0.04855692 0.06625253 0.10244857
## 0.04812782 0.06777401 0.09902507 0.04855692 0.06625253 0.10244855
sqrt(diag(solve(res$hessian)))


```






